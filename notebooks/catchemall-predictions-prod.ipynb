{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import streamlit as st\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file to examine its structure\n",
    "file_path = 'pokemon.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Displaying the first few rows of the dataframe to understand its structure\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "# import pickle\n",
    "\n",
    "# def preprocess_pokemon_data(df):\n",
    "#     # Drop unnecessary columns\n",
    "#     df = df.drop(columns=['japanese_name', 'name'])\n",
    "\n",
    "#     # Fill missing height and weight with their median\n",
    "#     df['height_m'] = df['height_m'].fillna(df['height_m'].median())\n",
    "#     df['weight_kg'] = df['weight_kg'].fillna(df['weight_kg'].median())\n",
    "\n",
    "#     # Encode 'classification' using One Hot Encoding\n",
    "#     ohe_classification = OneHotEncoder()\n",
    "#     classification_encoded = ohe_classification.fit_transform(df[['classfication']])\n",
    "#     with open('ohe_classification.pkl', 'wb') as f:\n",
    "#         pickle.dump(ohe_classification, f)\n",
    "\n",
    "#     # Correctly handle abilities list\n",
    "#     df['abilities'] = df['abilities'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "#     mlb_abilities = MultiLabelBinarizer()\n",
    "#     abilities_encoded = mlb_abilities.fit_transform(df['abilities'])\n",
    "#     with open('mlb_abilities.pkl', 'wb') as f:\n",
    "#         pickle.dump(mlb_abilities, f)\n",
    "\n",
    "#     # Combine and deduplicate 'type1' and 'type2'\n",
    "#     df['combined_types'] = df[['type1', 'type2']].apply(lambda x: list(set([y for y in x if pd.notna(y)])), axis=1)\n",
    "\n",
    "#     # One Hot Encoding for combined types\n",
    "#     mlb_types = MultiLabelBinarizer()\n",
    "#     types_encoded = mlb_types.fit_transform(df['combined_types'])\n",
    "#     with open('mlb_types.pkl', 'wb') as f:\n",
    "#         pickle.dump(mlb_types, f)\n",
    "\n",
    "#     # Drop the original 'classfication', 'abilities', 'type1', 'type2' columns\n",
    "#     df = df.drop(columns=['classfication', 'abilities', 'type1', 'type2', 'combined_types'])\n",
    "#     # Add encoded features back to dataframe\n",
    "#     classification_encoded_df = pd.DataFrame(classification_encoded.toarray(), columns=ohe_classification.get_feature_names_out())\n",
    "#     abilities_encoded_df = pd.DataFrame(abilities_encoded, columns=mlb_abilities.classes_)\n",
    "#     types_encoded_df = pd.DataFrame(types_encoded, columns=mlb_types.classes_)\n",
    "\n",
    "#     # Concatenate the encoded dataframes with the original dataframe\n",
    "#     df = pd.concat([df.reset_index(drop=True), classification_encoded_df, abilities_encoded_df, types_encoded_df], axis=1)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # Load the CSV file\n",
    "\n",
    "# pokemon_df = pd.read_csv(file_path)\n",
    "\n",
    "# # Preprocess the data\n",
    "# preprocessed_pokemon_df = preprocess_pokemon_data(pokemon_df)\n",
    "# preprocessed_pokemon_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Calculating the correlation matrix for the dataset\n",
    "# correlation_matrix = df.corr()\n",
    "\n",
    "# # Setting up the matplotlib figure\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# # Drawing a heatmap with the correlation matrix\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "\n",
    "# plt.title('Correlation Matrix for Pok√©mon Dataset')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns which are highly correlated to capture_rate\n",
    "base_total\n",
    "attack\n",
    "sp_attack\n",
    "sp_defense\n",
    "defense\n",
    "hp\n",
    "height_m\n",
    "speed\n",
    "weight_kg\n",
    "is_legendary\n",
    "base_egg_steps #0.87 corr with is_legendary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate creating new features or transforming existing ones that could be more informative.\n",
    "Conduct a more detailed feature selection process, possibly using techniques like Recursive Feature Elimination (RFE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some interaction features and apply RFE to select the most important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# pokemon_df_rfe = pd.read_csv(file_path)# Feature Engineering: Creating new interaction features\n",
    "# pokemon_df_rfe['attack_defense_ratio'] = pokemon_df_rfe['attack'] / pokemon_df_rfe['defense']\n",
    "# pokemon_df_rfe['total_stats'] = pokemon_df_rfe[['hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed']].sum(axis=1)\n",
    "# pokemon_df_rfe['weight_height_ratio'] = pokemon_df_rfe['weight_kg'] / pokemon_df_rfe['height_m']\n",
    "# # Correctly handle abilities list\n",
    "# pokemon_df_rfe['abilities'] = pokemon_df_rfe['abilities'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "# mlb_abilities = MultiLabelBinarizer()\n",
    "# abilities_encoded = mlb_abilities.fit_transform(pokemon_df_rfe['abilities'])\n",
    "# # Combine and deduplicate 'type1' and 'type2'\n",
    "# pokemon_df_rfe['combined_types'] = pokemon_df_rfe[['type1', 'type2']].apply(lambda x: list(set([y for y in x if pd.notna(y)])), axis=1)\n",
    "\n",
    "# # One Hot Encoding for combined types\n",
    "# mlb_types = MultiLabelBinarizer()\n",
    "# types_encoded = mlb_types.fit_transform(pokemon_df_rfe['combined_types'])\n",
    "# # Filling any infinite or NaN values created during feature engineering\n",
    "# pokemon_df_rfe.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# pokemon_df_rfe.fillna(pokemon_df_rfe.median(), inplace=True)\n",
    "\n",
    "#     # Drop the original 'classfication', 'abilities', 'type1', 'type2' columns\n",
    "# pokemon_df_rfe = pokemon_df_rfe.drop(columns=['classfication', 'abilities', 'type1', 'type2', 'combined_types', 'japanese_name', 'name'])\n",
    "\n",
    "# # Updating the list of numeric columns after adding new features\n",
    "# numeric_cols = pokemon_df_rfe.select_dtypes(include=['int64', 'float64']).columns\n",
    "# numeric_cols = numeric_cols.drop('capture_rate')\n",
    "\n",
    "# # Re-creating the numeric transformer and preprocessor\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_cols)])\n",
    "\n",
    "# # Splitting the data into features and target variable\n",
    "# X = pokemon_df_rfe.drop('capture_rate', axis=1)\n",
    "# y = pokemon_df_rfe['capture_rate'].apply(pd.to_numeric, errors='coerce')\n",
    "# y.fillna(y.median(), inplace=True)\n",
    "\n",
    "# # Applying Recursive Feature Elimination\n",
    "# linear_reg = LinearRegression()\n",
    "# rfe = RFECV(estimator=linear_reg, step=1, cv=5, scoring='neg_mean_squared_error')\n",
    "# X_rfe = preprocessor.fit_transform(X)\n",
    "# rfe.fit(X_rfe, y)\n",
    "\n",
    "# # Training a RandomForestRegressor model with selected features\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Selecting the features chosen by RFE\n",
    "# selected_features = [col for col, selected in zip(X.columns, rfe.support_) if selected]\n",
    "# X_selected = X[selected_features]\n",
    "\n",
    "# # Splitting data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Training the model\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting on test set and calculating the mean squared error\n",
    "# y_pred = model.predict(X_test)\n",
    "# new_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# new_mse\n",
    "# # OUTPUT 1887.716094375\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm it doesnt seem much better than before. Let's look at a different way, using some different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "\n",
    "def preprocess_pokemon_data(df):\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['japanese_name', 'name'])\n",
    "\n",
    "    # Fill missing values\n",
    "    df['height_m'] = df['height_m'].fillna(df['height_m'].median())\n",
    "    df['weight_kg'] = df['weight_kg'].fillna(df['weight_kg'].median())\n",
    "    df['type1'].fillna('None', inplace=True)\n",
    "    df['type2'].fillna('None', inplace=True)\n",
    "    df['abilities'].fillna('None', inplace=True)\n",
    "    df['classfication'].fillna('None', inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>against_bug</th>\n",
       "      <th>against_dark</th>\n",
       "      <th>against_dragon</th>\n",
       "      <th>against_electric</th>\n",
       "      <th>against_fairy</th>\n",
       "      <th>against_fight</th>\n",
       "      <th>against_fire</th>\n",
       "      <th>against_flying</th>\n",
       "      <th>against_ghost</th>\n",
       "      <th>against_grass</th>\n",
       "      <th>...</th>\n",
       "      <th>type2_ghost</th>\n",
       "      <th>type2_grass</th>\n",
       "      <th>type2_ground</th>\n",
       "      <th>type2_ice</th>\n",
       "      <th>type2_normal</th>\n",
       "      <th>type2_poison</th>\n",
       "      <th>type2_psychic</th>\n",
       "      <th>type2_rock</th>\n",
       "      <th>type2_steel</th>\n",
       "      <th>type2_water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 884 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   against_bug  against_dark  against_dragon  against_electric  against_fairy  \\\n",
       "0          1.0           1.0             1.0               0.5            0.5   \n",
       "1          1.0           1.0             1.0               0.5            0.5   \n",
       "2          1.0           1.0             1.0               0.5            0.5   \n",
       "3          0.5           1.0             1.0               1.0            0.5   \n",
       "4          0.5           1.0             1.0               1.0            0.5   \n",
       "\n",
       "   against_fight  against_fire  against_flying  against_ghost  against_grass  \\\n",
       "0            0.5           2.0             2.0            1.0           0.25   \n",
       "1            0.5           2.0             2.0            1.0           0.25   \n",
       "2            0.5           2.0             2.0            1.0           0.25   \n",
       "3            1.0           0.5             1.0            1.0           0.50   \n",
       "4            1.0           0.5             1.0            1.0           0.50   \n",
       "\n",
       "   ...  type2_ghost  type2_grass  type2_ground  type2_ice  type2_normal  \\\n",
       "0  ...          0.0          0.0           0.0        0.0           0.0   \n",
       "1  ...          0.0          0.0           0.0        0.0           0.0   \n",
       "2  ...          0.0          0.0           0.0        0.0           0.0   \n",
       "3  ...          0.0          0.0           0.0        0.0           0.0   \n",
       "4  ...          0.0          0.0           0.0        0.0           0.0   \n",
       "\n",
       "   type2_poison  type2_psychic  type2_rock  type2_steel  type2_water  \n",
       "0           1.0            0.0         0.0          0.0          0.0  \n",
       "1           1.0            0.0         0.0          0.0          0.0  \n",
       "2           1.0            0.0         0.0          0.0          0.0  \n",
       "3           0.0            0.0         0.0          0.0          0.0  \n",
       "4           0.0            0.0         0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 884 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_features(df):\n",
    "    \"\"\"\n",
    "    Function to encode various features in the Pokemon data.\n",
    "    \"\"\"\n",
    "    # Initialize encoders\n",
    "    ohe_classification = OneHotEncoder()\n",
    "    mlb_abilities = MultiLabelBinarizer()\n",
    "    ohe_type1 = OneHotEncoder()\n",
    "    ohe_type2 = OneHotEncoder()\n",
    "\n",
    "    # Perform encoding\n",
    "    classification_encoded = ohe_classification.fit_transform(df[['classfication']])\n",
    "    abilities_encoded = mlb_abilities.fit_transform(df['abilities'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \")))\n",
    "    type1_encoded = ohe_type1.fit_transform(df[['type1']])\n",
    "    type2_encoded = ohe_type2.fit_transform(df[['type2']])\n",
    "\n",
    "    # Save the fitted encoders using pickle\n",
    "    with open('ohe_classification.pkl', 'wb') as f:\n",
    "        pickle.dump(ohe_classification, f)\n",
    "    with open('mlb_abilities.pkl', 'wb') as f:\n",
    "        pickle.dump(mlb_abilities, f)\n",
    "    with open('ohe_type1.pkl', 'wb') as f:\n",
    "        pickle.dump(ohe_type1, f)\n",
    "    with open('ohe_type2.pkl', 'wb') as f:\n",
    "        pickle.dump(ohe_type2, f)\n",
    "\n",
    "    # Create DataFrames for encoded features\n",
    "    classification_encoded_df = pd.DataFrame(classification_encoded.toarray(), columns=ohe_classification.get_feature_names_out())\n",
    "    abilities_encoded_df = pd.DataFrame(abilities_encoded, columns=mlb_abilities.classes_)\n",
    "    type1_encoded_df = pd.DataFrame(type1_encoded.toarray(), columns=ohe_type1.get_feature_names_out(['type1']))\n",
    "    type2_encoded_df = pd.DataFrame(type2_encoded.toarray(), columns=ohe_type2.get_feature_names_out(['type2']))\n",
    "\n",
    "    # Combine encoded features with the original dataframe\n",
    "    df = pd.concat([df, classification_encoded_df, abilities_encoded_df, type1_encoded_df, type2_encoded_df], axis=1)\n",
    "\n",
    "    # Drop the original columns that were encoded\n",
    "    df = df.drop(columns=['classfication', 'abilities', 'type1', 'type2', 'percentage_male'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "pokemon_df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessed_pokemon_df = preprocess_pokemon_data(pokemon_df)\n",
    "\n",
    "# Encode features\n",
    "encoded_pokemon_df = encode_features(preprocessed_pokemon_df)\n",
    "\n",
    "# Display the encoded dataframe\n",
    "encoded_pokemon_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = encoded_pokemon_df.drop('capture_rate', axis=1)\n",
    "# y = encoded_pokemon_df['capture_rate']\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define models and their parameter grids for regression\n",
    "# models_param_grid = {\n",
    "#     RandomForestRegressor(random_state=42): {\n",
    "#         'n_estimators': [100, 200, 300],\n",
    "#         'max_depth': [10, 20, 30],\n",
    "#         'min_samples_split': [2, 5, 10]\n",
    "#     },\n",
    "#     XGBRegressor(random_state=42): {\n",
    "#         'n_estimators': [100, 200, 300],\n",
    "#         'learning_rate': [0.01, 0.05, 0.1],\n",
    "#         'max_depth': [3, 5, 7]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Randomized Search\n",
    "# best_models = {}\n",
    "# for model, param_grid in models_param_grid.items():\n",
    "#     random_search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "#     random_search.fit(X_train, y_train)\n",
    "#     best_models[model.__class__.__name__] = random_search.best_estimator_\n",
    "\n",
    "# # Evaluate the models\n",
    "# for model_name, model in best_models.items():\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     print(f\"Mean Squared Error for {model_name}: {mse:.2f}\")\n",
    "# # OUTPUT\n",
    "# # Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
    "# # Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
    "# # Mean Squared Error for RandomForestRegressor: 1419.58\n",
    "# # Mean Squared Error for XGBRegressor: 1574.71\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check data types of all columns\n",
    "# print(X_train.dtypes)\n",
    "\n",
    "# # Identify non-numeric columns\n",
    "# non_numeric_columns = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "# print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "# # If there are non-numeric columns, consider converting them to numeric or dropping them\n",
    "# # For example, to drop:\n",
    "# # X_train = X_train.drop(non_numeric_columns, axis=1)\n",
    "# # X_test = X_test.drop(non_numeric_columns, axis=1)\n",
    "\n",
    "# # Check for missing values\n",
    "# print(\"Missing values in X_train:\", X_train.isnull().sum().sum())\n",
    "\n",
    "# # Fill missing values or drop rows/columns with missing values\n",
    "# # For example, to fill with median:\n",
    "# # X_train = X_train.fillna(X_train.median())\n",
    "# # X_test = X_test.fillna(X_test.median())\n",
    "\n",
    "# # Reattempt the RandomizedSearchCV with the modified data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for missing values in each column in the training set\n",
    "# missing_values_train = X_train.isnull().sum()\n",
    "# print(\"Missing values in X_train:\")\n",
    "# print(missing_values_train[missing_values_train > 0])  # Display only columns with missing values\n",
    "\n",
    "# # Similarly, for the testing set\n",
    "# missing_values_test = X_test.isnull().sum()\n",
    "# print(\"\\nMissing values in X_test:\")\n",
    "# print(missing_values_test[missing_values_test > 0])  # Display only columns with missing values\n",
    "# # OUTPUT\n",
    "# # Missing values in X_train:\n",
    "# # Series([], dtype: int64)\n",
    "\n",
    "# # Missing values in X_test:\n",
    "# # Series([], dtype: int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Define the parameter grid based on the results of RandomizedSearchCV\n",
    "# param_grid = {\n",
    "#     'n_estimators': [150, 200, 250],  # Narrow down around the best found in RandomizedSearchCV\n",
    "#     'max_depth': [15, 20, 25],        # Narrow down around the best found in RandomizedSearchCV\n",
    "#     'min_samples_split': [3, 4, 5]    # Narrow down around the best found in RandomizedSearchCV\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV object\n",
    "# grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the corresponding score\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# print(\"Best score (MSE) from GridSearchCV: \", grid_search.best_score_)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error on Test Set: \", mse)\n",
    "# # OUTPUT\n",
    "# # Best parameters found:  {'max_depth': 25, 'min_samples_split': 5, 'n_estimators': 250}\n",
    "# # Best score (MSE) from GridSearchCV:  0.678780602243106\n",
    "# # Mean Squared Error on Test Set:  1426.991357061649\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found:  {'max_depth': 25, 'min_samples_split': 5, 'n_estimators': 250}\n",
    "Best score (MSE) from GridSearchCV:  0.678780602243106\n",
    "Mean Squared Error on Test Set:  1426.991357061649"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Define base models\n",
    "# base_models = [\n",
    "#     ('random_forest', RandomForestRegressor(n_estimators=2500, max_depth=25, min_samples_split=5, random_state=42)),\n",
    "#     ('knn', KNeighborsRegressor(n_neighbors=5))\n",
    "# ]\n",
    "\n",
    "# # Define final estimator\n",
    "# final_estimator = LinearRegression()\n",
    "\n",
    "# # Create the Stacking Regressor\n",
    "# stacked_regressor = StackingRegressor(estimators=base_models, final_estimator=final_estimator, n_jobs=-1)\n",
    "\n",
    "# # Fit the stacked model\n",
    "# stacked_regressor.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the stacked model\n",
    "# y_pred = stacked_regressor.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error for Stacked Model: \", mse)\n",
    "# # OUTPUT\n",
    "# # Mean Squared Error for Stacked Model:  1401.0486911916155\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Assume rf_best and xgb_best are your tuned RandomForestRegressor and XGBRegressor\n",
    "# rf_best = RandomForestRegressor(n_estimators=250, max_depth=25, min_samples_split=5, random_state=42)\n",
    "# xgb_best = XGBRegressor(n_estimators=250, max_depth=25, learning_rate=0.05, random_state=42)\n",
    "\n",
    "# # Train the models\n",
    "# rf_best.fit(X_train, y_train)\n",
    "# xgb_best.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# rf_predictions = rf_best.predict(X_test)\n",
    "# xgb_predictions = xgb_best.predict(X_test)\n",
    "\n",
    "# # Average the predictions\n",
    "# average_predictions = (rf_predictions + xgb_predictions) / 2\n",
    "\n",
    "# # Evaluate the ensemble\n",
    "# mse = mean_squared_error(y_test, average_predictions)\n",
    "# print(\"Mean Squared Error for Averaging Ensemble: \", mse)\n",
    "# # OUTPUT\n",
    "# # Mean Squared Error for Averaging Ensemble:  1502.0843498702257\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Mean Squared Error: 1470.9406802204612\n",
      "Mean Absolute Error: 26.343420934361188\n",
      "R-squared Score: 0.751598434078464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = encoded_pokemon_df.drop('capture_rate', axis=1)\n",
    "y = encoded_pokemon_df['capture_rate']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define base models for stacking\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=250, max_depth=25, min_samples_split=5, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "    ('svr', SVR()),  # Support Vector Regressor\n",
    "    ('knn', KNeighborsRegressor())  # K-Neighbors Regressor\n",
    "]\n",
    "\n",
    "# Initialize Stacking Regressor with a meta-regressor\n",
    "model = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=1.0))\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? investigate scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mondel interpretation (SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW FOR PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abilities</th>\n",
       "      <th>against_bug</th>\n",
       "      <th>against_dark</th>\n",
       "      <th>against_dragon</th>\n",
       "      <th>against_electric</th>\n",
       "      <th>against_fairy</th>\n",
       "      <th>against_fight</th>\n",
       "      <th>against_fire</th>\n",
       "      <th>against_flying</th>\n",
       "      <th>against_ghost</th>\n",
       "      <th>...</th>\n",
       "      <th>percentage_male</th>\n",
       "      <th>pokedex_number</th>\n",
       "      <th>sp_attack</th>\n",
       "      <th>sp_defense</th>\n",
       "      <th>speed</th>\n",
       "      <th>type1</th>\n",
       "      <th>type2</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>generation</th>\n",
       "      <th>is_legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Rock Head', 'Sturdy', 'Sand Veil', 'Magnet P...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99991</td>\n",
       "      <td>194.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Steel</td>\n",
       "      <td>Dark</td>\n",
       "      <td>999.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Swift Swim', 'Sniper', 'Damp']</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99992</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Thick Fat', 'Hydration', 'Ice Body']</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99993</td>\n",
       "      <td>102.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grass</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Early Bird', 'Flash Fire', 'Unnerve']</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99994</td>\n",
       "      <td>102.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Hustle', 'Scrappy', 'Moody']</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99995</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Steel</td>\n",
       "      <td>Dark</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           abilities  against_bug  \\\n",
       "0  ['Rock Head', 'Sturdy', 'Sand Veil', 'Magnet P...         2.00   \n",
       "1                   ['Swift Swim', 'Sniper', 'Damp']         0.25   \n",
       "2             ['Thick Fat', 'Hydration', 'Ice Body']         1.00   \n",
       "3            ['Early Bird', 'Flash Fire', 'Unnerve']         1.00   \n",
       "4                     ['Hustle', 'Scrappy', 'Moody']         0.50   \n",
       "\n",
       "   against_dark  against_dragon  against_electric  against_fairy  \\\n",
       "0          2.00            2.00              2.00           2.00   \n",
       "1          0.25            0.25              0.25           0.25   \n",
       "2          1.00            1.00              1.00           1.00   \n",
       "3          1.00            1.00              1.00           1.00   \n",
       "4          1.00            2.00              0.25           0.50   \n",
       "\n",
       "   against_fight  against_fire  against_flying  against_ghost  ...  \\\n",
       "0           2.00          2.00            2.00           2.00  ...   \n",
       "1           0.25          0.25            0.25           0.25  ...   \n",
       "2           1.00          1.00            1.00           1.00  ...   \n",
       "3           1.00          1.00            1.00           1.00  ...   \n",
       "4           1.00          2.00            0.25           1.00  ...   \n",
       "\n",
       "   percentage_male  pokedex_number  sp_attack  sp_defense  speed  type1  \\\n",
       "0              NaN           99991      194.0       230.0  180.0  Steel   \n",
       "1              NaN           99992       10.0        20.0    5.0   Fire   \n",
       "2              NaN           99993      102.0       125.0   97.0    NaN   \n",
       "3              NaN           99994      102.0       125.0   97.0    NaN   \n",
       "4              NaN           99995       10.0       100.0   10.0  Steel   \n",
       "\n",
       "     type2  weight_kg  generation  is_legendary  \n",
       "0     Dark      999.9         7.0           1.0  \n",
       "1  Psychic        0.1         NaN           0.0  \n",
       "2    Grass      500.0         NaN           0.0  \n",
       "3    Ghost      500.0         NaN           1.0  \n",
       "4     Dark       10.0         NaN           0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the new CSV file containing fake Pok√©mon data\n",
    "fake_pokemon_file_path = 'pokemon-prediction-added-fake-pokemons.csv'\n",
    "fake_pokemon_df = pd.read_csv(fake_pokemon_file_path, sep=\";\")\n",
    "\n",
    "# Displaying the first few rows of the dataframe to understand its structure\n",
    "fake_pokemon_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 42.58324273 173.61611786  46.81260708  39.551687    53.65500708\n",
      "  54.59397065 157.3269369  157.32693719]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the new data\n",
    "preprocessed_fake_pokemon_df = preprocess_pokemon_data(fake_pokemon_df)\n",
    "\n",
    "# Encode the new data using the same encoders\n",
    "encoded_fake_pokemon_df = encode_features(preprocessed_fake_pokemon_df)\n",
    "\n",
    "# Ensure the new data has the same columns as the training data, in the same order\n",
    "missing_cols = set(X_train.columns) - set(encoded_fake_pokemon_df.columns)\n",
    "missing_df = pd.DataFrame(0, index=np.arange(len(encoded_fake_pokemon_df)), columns=list(missing_cols))\n",
    "encoded_fake_pokemon_df = pd.concat([encoded_fake_pokemon_df, missing_df], axis=1)\n",
    "encoded_fake_pokemon_df = encoded_fake_pokemon_df[X_train.columns]\n",
    "\n",
    "# Ensure there are no NaN values in the data\n",
    "encoded_fake_pokemon_df.fillna(0, inplace=True)\n",
    "\n",
    "# Predict using the stacked model\n",
    "fake_pokemon_predictions = model.predict(encoded_fake_pokemon_df)\n",
    "\n",
    "# Display the predictions\n",
    "print(fake_pokemon_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a way of the user entering the 10 key attribute scores and for the others to be randomly generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Catchability: 223.6885173908982\n"
     ]
    }
   ],
   "source": [
    "def predict_catchability(base_total, attack, sp_attack, sp_defense, defense, hp, height_m, speed, weight_kg, is_legendary, base_egg_steps, model, original_df):\n",
    "    # Create a DataFrame for the input data\n",
    "    input_data = pd.DataFrame({\n",
    "        'base_total': [base_total],\n",
    "        'attack': [attack],\n",
    "        'sp_attack': [sp_attack],\n",
    "        'sp_defense': [sp_defense],\n",
    "        'defense': [defense],\n",
    "        'hp': [hp],\n",
    "        'height_m': [height_m],\n",
    "        'speed': [speed],\n",
    "        'weight_kg': [weight_kg],\n",
    "        'is_legendary': [is_legendary],\n",
    "        'base_egg_steps': [base_egg_steps]\n",
    "    })\n",
    "\n",
    "    # Generate random values for other attributes in a separate DataFrame\n",
    "    random_data = pd.DataFrame({column: np.random.choice(original_df[column].dropna().values, size=1)\n",
    "                                for column in original_df.columns if column not in input_data.columns})\n",
    "\n",
    "    # Combine the input and random data\n",
    "    input_data = pd.concat([input_data, random_data], axis=1)\n",
    "\n",
    "    # Ensure the input data has the same column order as the training data\n",
    "    input_data = input_data.reindex(columns=[col for col in original_df.columns if col != 'capture_rate'])\n",
    "\n",
    "    # Apply the prediction model\n",
    "    prediction = model.predict(input_data)\n",
    "\n",
    "    return prediction[0]\n",
    "\n",
    "# Example usage\n",
    "model = model  # Your trained model\n",
    "original_df = encoded_pokemon_df  # Original DataFrame\n",
    "\n",
    "# Example input values\n",
    "base_total = 10  # Example value\n",
    "attack = 10       # Example value\n",
    "sp_attack = 10    # Example value\n",
    "sp_defense = 10    # Example value\n",
    "defense = 10    # Example value\n",
    "hp = 10    # Example value\n",
    "height_m = 0.1    # Example value\n",
    "speed = 10    # Example value\n",
    "weight_kg = 1.0    # Example value\n",
    "is_legendary = 0    # Example value\n",
    "base_egg_steps = 20    # Example value\n",
    "\n",
    "# Predict\n",
    "catchability = predict_catchability(base_total,\n",
    "                                    attack,\n",
    "                                    sp_attack,\n",
    "                                    sp_defense,\n",
    "                                    defense,\n",
    "                                    hp,\n",
    "                                    height_m,\n",
    "                                    speed,\n",
    "                                    weight_kg,\n",
    "                                    is_legendary,\n",
    "                                    base_egg_steps,\n",
    "                                    model,\n",
    "                                    original_df)\n",
    "print(f\"Predicted Catchability: {catchability}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit to input 10 key attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I have migrated this to streamlit page 2new\n",
    "# # from your_model_import_script import model, original_df, predict_catchability  # Import your model and the predict function\n",
    "\n",
    "# # Streamlit page configuration\n",
    "# st.set_page_config(page_title=\"Pok√©mon Catchability Predictor\", layout=\"wide\")\n",
    "\n",
    "# # Streamlit interface\n",
    "# st.title(\"Pok√©mon Catchability Predictor\")\n",
    "\n",
    "# # Sidebar for input parameters\n",
    "# st.sidebar.header(\"Input Parameters\")\n",
    "# base_total = st.sidebar.slider(\"Base Total\", min_value=180, max_value=780, value=10, step=1)\n",
    "# attack = st.sidebar.slider(\"Attack\", min_value=180, max_value=780, value=10, step=1)\n",
    "# sp_attack = st.sidebar.slider(\"Special Attack\", min_value=0, max_value=200, value=10, step=1)\n",
    "# sp_defense = st.sidebar.slider(\"Special Defense\", min_value=0, max_value=200, value=10, step=1)\n",
    "# defense = st.sidebar.slider(\"Defense\", min_value=0, max_value=250, value=10, step=1)\n",
    "# hp = st.sidebar.slider(\"HP\", min_value=0, max_value=255, value=10, step=1)\n",
    "# height_m = st.sidebar.slider(\"Height (m)\", min_value=0.0, max_value=10.0, value=0.1, step=0.1)\n",
    "# speed = st.sidebar.slider(\"Speed\", min_value=0, max_value=200, value=10, step=1)\n",
    "# weight_kg = st.sidebar.slider(\"Weight (kg)\", min_value=0.0, max_value=1000.0, value=1.0, step=0.1)\n",
    "# is_legendary = st.sidebar.selectbox(\"Is Legendary\", options=[0, 1], format_func=lambda x: \"Yes\" if x == 1 else \"No\")\n",
    "# base_egg_steps = st.sidebar.slider(\"Base Egg Steps\", min_value=0, max_value=50000, value=20, step=100)\n",
    "\n",
    "# # Button for prediction\n",
    "# if st.button('Predict Catchability'):\n",
    "#     # Run prediction\n",
    "#     catchability = predict_catchability(base_total, attack, sp_attack, sp_defense, defense, hp, height_m, speed, weight_kg, is_legendary, base_egg_steps, model, original_df)\n",
    "#     st.success(f\"Predicted Catchability: {catchability}\")\n",
    "\n",
    "# # Instructions\n",
    "# st.write(\"Adjust the sliders to set the Pok√©mon's attributes and click 'Predict Catchability' to see the model's prediction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSOR\n",
    "\n",
    "# PREDICT\n",
    "\n",
    "# PIPELINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
